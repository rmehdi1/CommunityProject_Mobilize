import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
from typing import Dict, List, Tuple, Optional

class MessagingAnalyzer:
    """
    Utility class for analyzing petition messaging based on the research insights
    """
    
    def __init__(self):
        self.urgency_keywords = [
            'urgent', 'immediate', 'immediately', 'now', 'today', 'emergency', 'crisis',
            'deadline', 'time running out', "before it's too late", 'last chance',
            'act now', 'breaking', 'critical', 'asap', 'quickly', 'rapidly', 'soon'
        ]
        
        self.action_keywords = [
            'stop', 'save', 'protect', 'demand', 'fight', 'defend', 'prevent',
            'ban', 'end', 'cancel', 'reverse', 'change', 'fix', 'solve',
            'help', 'support', 'join', 'sign', 'act', 'take action', 'make'
        ]
        
        self.power_words = [
            'justice', 'freedom', 'rights', 'equality', 'fair', 'unfair', 'wrong',
            'illegal', 'violation', 'abuse', 'corruption', 'scandal', 'outrage',
            'discrimination', 'injustice', 'betrayal', 'exploitation', 'oppression'
        ]
        
        self.authority_keywords = [
            'government', 'minister', 'ministry', 'department', 'authority', 'official',
            'court', 'judge', 'police', 'administration', 'commissioner', 'director',
            'secretary', 'chief', 'president', 'prime minister', 'governor', 'congress'
        ]
    
    def analyze_content(self, title: str, description: str, letter_body: str = "") -> Dict:
        """
        Analyze petition content and return comprehensive metrics
        """
        results = {
            'title_analysis': self._analyze_text_component(title, 'title'),
            'description_analysis': self._analyze_text_component(description, 'description'),
            'letter_analysis': self._analyze_text_component(letter_body, 'letter') if letter_body else {},
            'overall_score': 0,
            'recommendations': []
        }
        
        # Calculate overall score
        results['overall_score'] = self._calculate_overall_score(results)
        
        # Generate recommendations
        results['recommendations'] = self._generate_recommendations(results)
        
        return results
    
    def _analyze_text_component(self, text: str, component_type: str) -> Dict:
        """Analyze individual text component"""
        if not text:
            return {}
        
        analysis = {
            'length': len(text),
            'word_count': len(text.split()),
            'sentence_count': len(re.split(r'[.!?]+', text)),
            'urgency_keywords': self._count_keywords(text, self.urgency_keywords),
            'action_keywords': self._count_keywords(text, self.action_keywords),
            'power_words': self._count_keywords(text, self.power_words),
            'authority_keywords': self._count_keywords(text, self.authority_keywords),
            'html_tags': len(re.findall(r'<[^>]+>', text)),
            'has_numbers': bool(re.search(r'\d+', text)),
            'readability_estimate': self._estimate_readability(text)
        }
        
        # Component-specific benchmarks
        benchmarks = self._get_benchmarks(component_type)
        analysis['meets_length_target'] = analysis['length'] >= benchmarks['min_length']
        analysis['optimization_score'] = self._calculate_component_score(analysis, benchmarks)
        
        return analysis
    
    def _count_keywords(self, text: str, keywords: List[str]) -> int:
        """Count occurrences of keywords in text"""
        text_lower = text.lower()
        count = 0
        for keyword in keywords:
            count += text_lower.count(keyword.lower())
        return count
    
    def _estimate_readability(self, text: str) -> str:
        """Estimate readability level based on sentence and word complexity"""
        if not text:
            return "Unknown"
        
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        
        if len(sentences) == 0:
            return "Unknown"
        
        avg_words_per_sentence = len(words) / len(sentences)
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        
        # Simple readability estimation
        if avg_words_per_sentence > 20 and avg_word_length > 6:
            return "Graduate Level"
        elif avg_words_per_sentence > 15 and avg_word_length > 5:
            return "College Level"
        elif avg_words_per_sentence > 10:
            return "High School Level"
        else:
            return "Elementary Level"
    
    def _get_benchmarks(self, component_type: str) -> Dict:
        """Get optimization benchmarks for each component type"""
        benchmarks = {
            'title': {
                'min_length': 80,
                'optimal_length': 83,
                'min_keywords': 2,
                'target_readability': 'Graduate Level'
            },
            'description': {
                'min_length': 1500,
                'optimal_length': 1511,
                'min_html_tags': 25,
                'min_keywords': 5,
                'target_readability': 'Graduate Level'
            },
            'letter': {
                'min_length': 65,
                'optimal_length': 66,
                'min_keywords': 1,
                'target_readability': 'College Level'
            }
        }
        return benchmarks.get(component_type, {})
    
    def _calculate_component_score(self, analysis: Dict, benchmarks: Dict) -> float:
        """Calculate optimization score for a component"""
        if not benchmarks:
            return 0.0
        
        score = 0.0
        max_score = 100.0
        
        # Length score (30 points)
        if analysis['length'] >= benchmarks.get('min_length', 0):
            score += 30
        else:
            score += (analysis['length'] / benchmarks.get('min_length', 1)) * 30
        
        # Keyword score (40 points)
        total_keywords = (analysis['urgency_keywords'] + analysis['action_keywords'] + 
                         analysis['power_words'] + analysis['authority_keywords'])
        min_keywords = benchmarks.get('min_keywords', 1)
        if total_keywords >= min_keywords:
            score += 40
        else:
            score += (total_keywords / min_keywords) * 40
        
        # HTML formatting score (20 points for descriptions)
        if 'min_html_tags' in benchmarks:
            if analysis['html_tags'] >= benchmarks['min_html_tags']:
                score += 20
            else:
                score += (analysis['html_tags'] / benchmarks['min_html_tags']) * 20
        else:
            score += 20  # Full points for non-description components
        
        # Readability score (10 points)
        if analysis['readability_estimate'] in ['Graduate Level', 'College Level']:
            score += 10
        elif analysis['readability_estimate'] == 'High School Level':
            score += 5
        
        return min(score, max_score)
    
    def _calculate_overall_score(self, results: Dict) -> float:
        """Calculate overall optimization score"""
        component_scores = []
        
        if results['title_analysis']:
            component_scores.append(results['title_analysis']['optimization_score'] * 0.25)
        
        if results['description_analysis']:
            component_scores.append(results['description_analysis']['optimization_score'] * 0.6)
        
        if results['letter_analysis']:
            component_scores.append(results['letter_analysis']['optimization_score'] * 0.15)
        
        return sum(component_scores) if component_scores else 0.0
    
    def _generate_recommendations(self, results: Dict) -> List[str]:
        """Generate specific recommendations based on analysis"""
        recommendations = []
        
        # Title recommendations
        if results['title_analysis']:
            title = results['title_analysis']
            if title['length'] < 80:
                recommendations.append("ðŸ”¸ Expand title to 80+ characters for optimal impact")
            if title['authority_keywords'] == 0:
                recommendations.append("ðŸ”¸ Include specific authority/institution references in title")
            if title['urgency_keywords'] == 0 and title['action_keywords'] == 0:
                recommendations.append("ðŸ”¸ Add urgency or action keywords to title")
        
        # Description recommendations
        if results['description_analysis']:
            desc = results['description_analysis']
            if desc['length'] < 1500:
                recommendations.append("ðŸ”¹ Expand description to 1,500+ characters for maximum effectiveness")
            if desc['html_tags'] < 25:
                recommendations.append("ðŸ”¹ Add professional HTML formatting (target: 25+ tags)")
            if desc['power_words'] == 0:
                recommendations.append("ðŸ”¹ Include power words (justice, rights, equality) for emotional impact")
            if not desc['has_numbers']:
                recommendations.append("ðŸ”¹ Include statistics or data to support your case")
        
        # Letter recommendations
        if results['letter_analysis'] and results['letter_analysis'].get('length', 0) > 0:
            letter = results['letter_analysis']
            if letter['length'] < 65:
                recommendations.append("ðŸ”¸ Expand letter body to 65+ characters with specific requests")
            if letter['authority_keywords'] == 0:
                recommendations.append("ðŸ”¸ Address specific officials by name and title in letter")
        
        # Overall recommendations
        overall_score = results['overall_score']
        if overall_score < 60:
            recommendations.append("âš ï¸ Overall content needs significant improvement for optimal success probability")
        elif overall_score < 80:
            recommendations.append("ðŸ“ˆ Content is good but has room for optimization")
        else:
            recommendations.append("âœ… Content meets high-performance standards!")
        
        return recommendations


def create_content_analyzer_widget():
    """Create an interactive content analyzer widget"""
    st.subheader("ðŸ” Live Content Analyzer")
    st.write("Analyze your petition content against our success framework:")
    
    # Initialize analyzer
    analyzer = MessagingAnalyzer()
    
    # Input fields
    title_input = st.text_area(
        "Petition Title", 
        placeholder="Enter your petition title here...",
        height=100
    )
    
    description_input = st.text_area(
        "Petition Description",
        placeholder="Enter your detailed petition description here...",
        height=200
    )
    
    letter_input = st.text_area(
        "Letter Body (Optional)",
        placeholder="Enter the letter body to officials...",
        height=100
    )
    
    if st.button("ðŸ” Analyze Content", type="primary"):
        if title_input or description_input:
            # Perform analysis
            analysis_results = analyzer.analyze_content(title_input, description_input, letter_input)
            
            # Display overall score
            col1, col2, col3 = st.columns(3)
            
            with col1:
                score = analysis_results['overall_score']
                color = "normal" if score >= 80 else "inverse" if score >= 60 else "off"
                st.metric(
                    label="Overall Optimization Score",
                    value=f"{score:.1f}%",
                    delta="Target: 80%+"
                )
            
            with col2:
                if analysis_results['title_analysis']:
                    title_score = analysis_results['title_analysis']['optimization_score']
                    st.metric(
                        label="Title Score",
                        value=f"{title_score:.1f}%",
                        delta=f"Length: {analysis_results['title_analysis']['length']} chars"
                    )
            
            with col3:
                if analysis_results['description_analysis']:
                    desc_score = analysis_results['description_analysis']['optimization_score']
                    st.metric(
                        label="Description Score", 
                        value=f"{desc_score:.1f}%",
                        delta=f"Length: {analysis_results['description_analysis']['length']} chars"
                    )
            
            # Detailed analysis
            st.subheader("ðŸ“Š Detailed Analysis")
            
            # Component analysis tabs
            if analysis_results['title_analysis'] or analysis_results['description_analysis']:
                tab1, tab2, tab3 = st.tabs(["Title Analysis", "Description Analysis", "Recommendations"])
                
                with tab1:
                    if analysis_results['title_analysis']:
                        display_component_analysis("Title", analysis_results['title_analysis'])
                    else:
                        st.info("No title provided for analysis")
                
                with tab2:
                    if analysis_results['description_analysis']:
                        display_component_analysis("Description", analysis_results['description_analysis'])
                    else:
                        st.info("No description provided for analysis")
                
                with tab3:
                    st.write("**Optimization Recommendations:**")
                    for rec in analysis_results['recommendations']:
                        st.write(rec)
                    
                    # Success probability estimate
                    score = analysis_results['overall_score']
                    if score >= 80:
                        st.success("ðŸŽ¯ **High Success Probability**: Content follows best practices for petition success")
                    elif score >= 60:
                        st.warning("ðŸ“ˆ **Moderate Success Probability**: Good foundation with room for improvement")
                    else:
                        st.error("âš ï¸ **Low Success Probability**: Significant optimization needed")
        else:
            st.warning("Please enter at least a title or description to analyze.")


def display_component_analysis(component_name: str, analysis: Dict):
    """Display detailed analysis for a content component"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.write(f"**{component_name} Metrics:**")
        st.write(f"â€¢ Length: {analysis['length']} characters")
        st.write(f"â€¢ Word Count: {analysis['word_count']} words")
        st.write(f"â€¢ Readability: {analysis['readability_estimate']}")
        if 'html_tags' in analysis and analysis['html_tags'] > 0:
            st.write(f"â€¢ HTML Tags: {analysis['html_tags']}")
    
    with col2:
        st.write(f"**Keyword Analysis:**")
        st.write(f"â€¢ Urgency Keywords: {analysis['urgency_keywords']}")
        st.write(f"â€¢ Action Keywords: {analysis['action_keywords']}")
        st.write(f"â€¢ Power Words: {analysis['power_words']}")
        st.write(f"â€¢ Authority Terms: {analysis['authority_keywords']}")
    
    # Optimization status
    if analysis['meets_length_target']:
        st.success(f"âœ… {component_name} meets minimum length requirements")
    else:
        benchmarks = {
            'Title': 80,
            'Description': 1500,
            'Letter': 65
        }
        target = benchmarks.get(component_name, 0)
        st.error(f"âŒ {component_name} below minimum length (target: {target}+ characters)")


def create_success_probability_predictor():
    """Create a success probability prediction tool"""
    st.subheader("ðŸŽ¯ Success Probability Predictor")
    st.write("Input petition characteristics to estimate success probability:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Content Characteristics:**")
        title_length = st.slider("Title Length (characters)", 20, 200, 83)
        desc_length = st.slider("Description Length (characters)", 100, 3000, 1511)
        html_tags = st.slider("HTML Formatting Tags", 0, 50, 25)
        
    with col2:
        st.write("**Strategic Elements:**")
        authority_refs = st.slider("Authority References", 0, 10, 3)
        action_keywords = st.slider("Action Keywords", 0, 15, 5)
        has_data = st.checkbox("Includes Statistics/Data", value=True)
        professional_tone = st.checkbox("Professional/Formal Tone", value=True)
    
    # Calculate success probability based on research insights
    base_probability = 23.2  # Baseline success rate
    
    # Length adjustments (based on quartile analysis)
    if title_length >= 83:
        base_probability *= 1.3  # Long titles perform better
    
    if desc_length >= 1511:
        base_probability *= 1.65  # Very long descriptions have highest success
    elif desc_length >= 1000:
        base_probability *= 1.3
    
    # HTML formatting impact
    if html_tags >= 25:
        base_probability *= 1.5  # Professional formatting advantage
    elif html_tags >= 10:
        base_probability *= 1.2
    
    # Strategic elements
    if authority_refs >= 3:
        base_probability *= 1.15
    
    if action_keywords >= 5:
        base_probability *= 1.1
    
    if has_data:
        base_probability *= 1.05
    
    if professional_tone:
        base_probability *= 1.1
    
    # Cap at reasonable maximum
    predicted_probability = min(base_probability, 85.0)
    
    # Display prediction
    st.subheader("ðŸ“ˆ Prediction Results")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="Predicted Success Rate",
            value=f"{predicted_probability:.1f}%",
            delta=f"+{predicted_probability - 23.2:.1f}pp vs baseline"
        )
    
    with col2:
        performance_level = "High" if predicted_probability >= 35 else "Moderate" if predicted_probability >= 25 else "Low"
        st.metric(
            label="Performance Level",
            value=performance_level,
            delta="Based on optimization"
        )
    
    with col3:
        improvement_factor = predicted_probability / 23.2
        st.metric(
            label="Improvement Factor",
            value=f"{improvement_factor:.1f}x",
            delta="vs average petition"
        )
    
    # Recommendations based on prediction
    if predicted_probability >= 40:
        st.success("ðŸŽ¯ **Excellent**: This petition configuration has high success potential!")
    elif predicted_probability >= 30:
        st.warning("ðŸ“ˆ **Good**: Solid foundation with strong success indicators")
    else:
        st.error("âš ï¸ **Needs Improvement**: Consider optimizing key elements for better success probability")


# Additional utility functions for the main app
def load_example_petitions() -> pd.DataFrame:
    """Load example petition data for demonstration"""
    examples = {
        'Title': [
            'Mandatory Installation of Oxygen Plants in All Hospitals Above 50 Beds',
            'Department of Health: Implement Comprehensive COVID-19 Safety Protocols',
            'City Council: Establish Emergency Housing Program for Homeless Population'
        ],
        'Success_Rate': [85, 72, 68],
        'Length': [83, 94, 89],
        'Authority_References': [2, 3, 2],
        'Professional_Score': [92, 88, 85]
    }
    return pd.DataFrame(examples)


def create_case_study_examples():
    """Create case study examples tab"""
    st.subheader("ðŸ“š Success Case Studies")
    
    case_studies = {
        "High-Impact Healthcare Petition": {
            "title": "Mandatory Installation of Oxygen Plants in All Hospitals Above 50 Beds",
            "success_rate": "High Success Probability",
            "key_factors": [
                "Specific, measurable demand (50+ beds threshold)",
                "Authority targeting (healthcare institutions)",
                "Technical precision (oxygen plants vs. general equipment)",
                "Implementation clarity (mandatory installation)"
            ],
            "length_analysis": "83 characters - meets optimal length benchmark",
            "lessons": "Specificity and technical precision drive credibility"
        },
        "Effective Government Policy Request": {
            "title": "Department of Health: Implement 24/7 Emergency Response Protocol",
            "success_rate": "High Success Probability", 
            "key_factors": [
                "Direct authority addressing (Department of Health)",
                "Clear timeline specification (24/7)",
                "Professional terminology (emergency response protocol)",
                "Actionable implementation (implement vs. consider)"
            ],
            "length_analysis": "94 characters - exceeds minimum requirements",
            "lessons": "Authority specificity and clear timelines enhance effectiveness"
        }
    }
    
    for study_name, details in case_studies.items():
        with st.expander(f"ðŸ“– {study_name}"):
            st.write(f"**Title:** {details['title']}")
            st.write(f"**Success Prediction:** {details['success_rate']}")
            st.write(f"**Length Analysis:** {details['length_analysis']}")
            
            st.write("**Key Success Factors:**")
            for factor in details['key_factors']:
                st.write(f"â€¢ {factor}")
            
            st.info(f"**Key Lesson:** {details['lessons']}")


# Export all functions for use in main app
__all__ = [
    'MessagingAnalyzer',
    'create_content_analyzer_widget', 
    'create_success_probability_predictor',
    'create_case_study_examples',
    'load_example_petitions'
]